for comparison it is evident that a random classifier would have 25% of accuracy, bc we have value for the label


test k_neighbourg for 1 < k < 500 , weights = 'uniform', and without normalization

  with gray average only : 49 %  with k = 296 (best)
  with gray entropy only : 51 %  with k = 101 (best)
  with gray rms only : 50 %  with k = 391  (best)
  with gray kurtosis only : 50 %  with k = 206 (best)
  with gray skewness only : 49 %  with k = 311 (best)
  with gray std only : 52 %  with k = 91 (best)

  with entropy and skewness : 53 % with k = 126 (best)
  with entropy skewness std rms : 59 % with k = 26 (best)

  with the 6 features : 62 % with k = 16 (best)


test k_neighbourg for 1 < k < 500 , weights = 'distance', and without normalization

  with the 6 features : 64 % with k = 19(best)


test k_neighbourg for 1 < k < 500 , weights = 'uniform', and with normalization

  with the 6 features : 55 % with k = 481 (accuracy keep increasing very smoothly wiht k, seems to be bounded by 56%)


test k_neighbourg for 1 < k < 500 , weights = 'distance', and with normalization

  with the 6 features : 55 % with k = 496 (accuracy keep increasing very smoothly wiht k, seems to be bounded by 56%)



conclusion :
    normalization should have upgrade our accuracy but it is not the case
    the use of weighted knn does not change our results in normalization or not normalization

issues :
    the lables are not uniformly sample

next step :
    try with the wavelet and shearlet features but curse of dimensionnality will surely occur
    try with radius KNN to deal with the problems of non uniformaly samples labels

sources of information :
    https://www.datacamp.com/community/tutorials/k-nearest-neighbor-classification-scikit-learn
    https://scikit-learn.org/stable/modules/neighbors.html
